{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STACKING ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sped up stacking analysis using numba.jit decorator with nopython=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import multiprocessing as mul\n",
    "from multiprocessing import Process\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "import itertools as itrt\n",
    "from scipy.optimize import minimize\n",
    "from scipy.optimize import curve_fit\n",
    "from IPython.display import clear_output\n",
    "from itertools import repeat\n",
    "from functools import partial\n",
    "from scipy.stats import chi2\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import gaussian_kde\n",
    "import scipy.stats as sct\n",
    "from astropy.coordinates import SkyCoord as scr\n",
    "from astropy import units as u\n",
    "from time import sleep\n",
    "from numba import jit, njit, prange\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read icdata\n",
      "IC40_exp.csv read\n",
      "IC59_exp.csv read\n",
      "IC79_exp.csv read\n",
      "IC86_I_exp.csv read\n",
      "IC86_II_exp.csv read\n",
      "IC86_III_exp.csv read\n",
      "IC86_IV_exp.csv read\n",
      "IC86_V_exp.csv read\n",
      "IC86_VI_exp.csv read\n",
      "IC86_VII_exp.csv read\n",
      "IC40_effectiveArea.csv read\n",
      "IC59_effectiveArea.csv read\n",
      "IC79_effectiveArea.csv read\n",
      "IC86_I_effectiveArea.csv read\n",
      "IC86_II_effectiveArea.csv read\n"
     ]
    }
   ],
   "source": [
    "####\n",
    "#### IMPORTING AND SPLITTING ICDATA $$$\n",
    "\n",
    "\n",
    "path = \"/media/darkwake/VIB2/Project-IceCube/icecube_10year_ps/events\"\n",
    "t_eff_path = \"/media/darkwake/VIB2/Project-IceCube/icecube_10year_ps/uptime\"\n",
    "irf_path = \"/media/darkwake/VIB2/Project-IceCube/icecube_10year_ps/irfs\"\n",
    "filenames = [\"IC40_exp.csv\", \"IC59_exp.csv\",\"IC79_exp.csv\", \"IC86_I_exp.csv\", \"IC86_II_exp.csv\",\n",
    "\"IC86_III_exp.csv\", \"IC86_IV_exp.csv\", \"IC86_V_exp.csv\", \"IC86_VI_exp.csv\", \"IC86_VII_exp.csv\"]\n",
    "file = filenames[0]\n",
    "f = open(os.path.join(path, file), 'r')\n",
    "lines = f.readlines()\n",
    "column=lines[0].split()\n",
    "column.pop(0)\n",
    "content = []\n",
    "for file in filenames:\n",
    "    f = open(os.path.join(path, file), 'r')\n",
    "    lines = f.readlines()\n",
    "    #print(len(lines) - 1)\n",
    "    for line in lines[1:]:\n",
    "        content.append(line.split())\n",
    "    f.close()\n",
    "icdata = pd.DataFrame(content, columns=column, dtype=float)#.convert_dtypes(infer_objects=True,convert_integer=True,convert_floating=True)\n",
    "icdata['log10(E/GeV)'] = [float(i) for i in icdata['log10(E/GeV)']]\n",
    "icdata['MJD[days]'] = [float(i) for i in icdata['MJD[days]']]\n",
    "\n",
    "print(\"read icdata\")\n",
    "f.close()\n",
    "\n",
    "\n",
    "\n",
    "#Importing UPtime data\n",
    "file = filenames[0]\n",
    "f = open(os.path.join(t_eff_path, file), 'r')\n",
    "lines = f.readlines()\n",
    "column=lines[0].split()\n",
    "column.pop(0)\n",
    "uptdata = []\n",
    "for file in filenames:\n",
    "    content = []\n",
    "    f = open(os.path.join(t_eff_path, file), 'r')\n",
    "    lines = f.readlines()\n",
    "    for line in lines[1:]:\n",
    "        content.append(line.split())\n",
    "    f.close()\n",
    "    temp = pd.DataFrame(content, columns=column)\n",
    "    temp['MJD_start[days]'] = [float(i) for i in temp['MJD_start[days]']]\n",
    "    temp['MJD_stop[days]'] = [float(i) for i in temp['MJD_stop[days]']]\n",
    "    uptdata.append(temp)\n",
    "    temp = []\n",
    "    content = []\n",
    "    print(file + \" read\")\n",
    "f.close()\n",
    "\n",
    "\n",
    "#Importing Aeff data\n",
    "filenames = [\"IC40_effectiveArea.csv\", \"IC59_effectiveArea.csv\",\"IC79_effectiveArea.csv\", \"IC86_I_effectiveArea.csv\", \"IC86_II_effectiveArea.csv\"]\n",
    "file = filenames[0]\n",
    "f = open(os.path.join(irf_path, file), 'r')\n",
    "lines = f.readlines()\n",
    "column=lines[0].split()\n",
    "column.pop(0)\n",
    "eadata = []\n",
    "for file in filenames:\n",
    "    content = []\n",
    "    f = open(os.path.join(irf_path, file), 'r')\n",
    "    lines = f.readlines()\n",
    "    for line in lines[1:]:\n",
    "        content.append(line.split())\n",
    "    f.close()\n",
    "    temp = pd.DataFrame(content, columns=column, dtype=float)\n",
    "    #temp['MJD_start[days]'] = [float(i) for i in temp['MJD_start[days]']]\n",
    "    #temp['MJD_stop[days]'] = [float(i) for i in temp['MJD_stop[days]']]\n",
    "    eadata.append(temp)\n",
    "    temp = []\n",
    "    content = []\n",
    "    print(file + \" read\")\n",
    "f.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For calculating Weights effectiveArea files are used\n",
    "\n",
    "#### All effectiveArea files differ only in the effectiveArea column.\n",
    "\n",
    "#### Rest of the columns are the same.\n",
    "\n",
    "#### Energy log(E/GeV) ranges from 2 to 10 in steps of 0.2\n",
    "\n",
    "#### So e_nu = midpoint of (2, 2.2), (2.2, 2.4), (2.4, 2.6) and so on.\n",
    "\n",
    "#### e_nu = $10^9 * 10^{(2.1, 2.3, 2.5, .... 9.9)}$\n",
    "\n",
    "#### de_nu = $10^9 * (10^{(2.2, 2.4, .... 10)} - 10^{(2, 2.2, .... 9.8)})$\n",
    "\n",
    "#### Each energy bin has declination range from -90 to 90 in 51 bins.\n",
    "\n",
    "#### So these variables are generic for all effectiveArea files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15, 39]\n"
     ]
    }
   ],
   "source": [
    "icwidths = [int(i) for i in \"0 36900 107011 93133 136244 112858 122541 127045 129311 123657 145750\".split(' ')]\n",
    "ictimes = [float(i) for i in icdata['MJD[days]']]\n",
    "icparts = [np.sum(icwidths[:i]) for i in range(1,len(icwidths)+1)]  #paritions of icdata for each season (IC40, IC59, IC79, IC86_I, IC86_II)\n",
    "#icparts.pop(-1)\n",
    "#icparts\n",
    "upt_icparts = icparts[:5]\n",
    "upt_icparts.append(icparts[-1])\n",
    "#len(upt_icparts) - 1 == len(eadata)\n",
    "\n",
    "log_e = np.round(np.arange(2, 10.2, 0.2), 2) #log10(E/GeV) values range as in all 'effectiveArea' files\n",
    "\n",
    "#dec_nu = mid point of Dec_nu_min and Dec_nu_max as in all 'effectiveArea' files\n",
    "dec_nu = list(set(eadata[0]['Dec_nu_min[deg]'].values).union(set(eadata[0]['Dec_nu_max[deg]'].values)))\n",
    "\n",
    "dec_nu.sort()\n",
    "dec_nu = np.array(dec_nu)\n",
    "\n",
    "e_nu = ((10**(log_e[:-1])+ 10**(log_e[1:]))/2)*1e9\n",
    "#de_nu = 1e9**0.2\n",
    "de_nu = 1e9*(10**log_e[1:] - 10**log_e[:-1])\n",
    "#Test case for e_ind\n",
    "nu_e = 5\n",
    "e_ind = 0\n",
    "for i in range(len(log_e) - 1):\n",
    "    if nu_e >= log_e[i] and nu_e < log_e[i+1]:\n",
    "        e_ind = i\n",
    "print([e_ind, i])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing ATNF Pulsar data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NAME</th>\n",
       "      <th>RAJD</th>\n",
       "      <th>DECJD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>J0002+6216</td>\n",
       "      <td>0.74238</td>\n",
       "      <td>62.26928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>J0006+1834</td>\n",
       "      <td>1.52</td>\n",
       "      <td>18.5831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>J0007+7303</td>\n",
       "      <td>1.7571</td>\n",
       "      <td>73.0521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>J0011+08</td>\n",
       "      <td>2.9</td>\n",
       "      <td>8.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B0011+47</td>\n",
       "      <td>3.57396</td>\n",
       "      <td>47.77594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3336</th>\n",
       "      <td>J2351+8533</td>\n",
       "      <td>357.764</td>\n",
       "      <td>85.55573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3337</th>\n",
       "      <td>J2352+65</td>\n",
       "      <td>358</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3338</th>\n",
       "      <td>J2354-22</td>\n",
       "      <td>358.60833</td>\n",
       "      <td>-22.86472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3339</th>\n",
       "      <td>B2351+61</td>\n",
       "      <td>358.5199292</td>\n",
       "      <td>61.9296792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3340</th>\n",
       "      <td>J2355+2246</td>\n",
       "      <td>358.9575</td>\n",
       "      <td>22.771</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3341 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            NAME         RAJD       DECJD\n",
       "0     J0002+6216      0.74238    62.26928\n",
       "1     J0006+1834         1.52     18.5831\n",
       "2     J0007+7303       1.7571     73.0521\n",
       "3       J0011+08          2.9        8.17\n",
       "4       B0011+47      3.57396    47.77594\n",
       "...          ...          ...         ...\n",
       "3336  J2351+8533      357.764    85.55573\n",
       "3337    J2352+65          358          65\n",
       "3338    J2354-22    358.60833   -22.86472\n",
       "3339    B2351+61  358.5199292  61.9296792\n",
       "3340  J2355+2246     358.9575      22.771\n",
       "\n",
       "[3341 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#IMPORTING MSPDATA\n",
    "f = open(\"/media/darkwake/VIB2/Project-IceCube/allpsr1.68.txt\", 'r')\n",
    "lines = f.readlines()\n",
    "content=[]\n",
    "column=lines.pop(0).replace('x', '').replace('#', '').split()\n",
    "for line in lines[:]:\n",
    "    content.append(line.split())\n",
    "    #the INITAL DATABASE IS CLUTTERED SO WE REMOVE THE NULL COLUMNS AND OTHER CLUTTER\n",
    "f.close()\n",
    "mspdata = pd.DataFrame(content).drop(0, axis=1)#.dropna()#.drop_duplicates()#.drop(range(0,6)).dropna()\n",
    "\n",
    "line = []\n",
    "lines = []\n",
    "mspdata.columns = column\n",
    "column = []\n",
    "content=[]\n",
    "#mspdata = mspdata.sort_values('DECJD')\n",
    "mspdata.dropna(inplace=True)\n",
    "#mspdata = mspdata.reset_index()\n",
    "#mspdata = mspdata.drop(\"index\", axis=1)\n",
    "mspdata"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorizing important & frequently used values\n",
    "\n",
    "#### msra/dec store the ra/dec values of the pulsars\n",
    "\n",
    "#### icra/dec store the ra/dec values of the neutrino events\n",
    "\n",
    "#### icang stores the angular uncertainity\n",
    "\n",
    "#### iceng stores the log(energy/GeV) of the neutrino events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-89.957, 89.977]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msra = np.array([float(i) for i in mspdata['RAJD'].values])\n",
    "msdec = np.array([float(i) for i in mspdata['DECJD'].values])\n",
    "icra = np.array([float(i) for i in icdata['RA[deg]']])\n",
    "icdec = np.array([float(i) for i in icdata['Dec[deg]']])\n",
    "icang = np.array([float(i) for i in icdata['AngErr[deg]']])\n",
    "iceng = np.array([float(i) for i in icdata['log10(E/GeV)']])\n",
    "global p, lg, lnu\n",
    "p = len(msra)\n",
    "lg = len(icra) // p + 1\n",
    "lnu = len(icra)\n",
    "[min(icdec), max(icdec)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "upstop_ttt = np.asfarray([uptdata[i]['MJD_stop[days]'].values[-1] for i in range(len(uptdata))])\n",
    "upstart_ttt = np.asfarray([uptdata[i]['MJD_start[days]'].values[0] for i in range(len(uptdata))])\n",
    "earea = np.array([eadata[i]['A_Eff[cm^2]'].values for i in range(len(eadata))]) * 1e-4\n",
    "vec_uptparts = np.asarray(upt_icparts, dtype=np.int64)\n",
    "upt_icparts = np.asarray(upt_icparts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "deg2rad_var = np.pi/180\n",
    "@jit(nopython=True, fastmath=True)\n",
    "def hvovec(lon1=0.0, lat1=0.0, lon2=0.0, lat2=0.0, rad=False):\n",
    "\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    lon1 : float\n",
    "        Longitude of first point.\n",
    "    lat1 : float\n",
    "        Latitude of first point.\n",
    "\n",
    "\n",
    "    lon2 : float\n",
    "        Longitude of second point.\n",
    "    lat2 : float\n",
    "        Latitude of second point.\n",
    "\n",
    "    rad : Boolean, optional\n",
    "        (default) False -> Angle is returned in Degrees\n",
    "        True -> Angle is returned in radians\n",
    "    \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Returns the haversine angle between two vectors given their latitude and longitude\n",
    "    \n",
    "\n",
    "    Warning\n",
    "    -------\n",
    "        This function assumes the input to be in degrees and of equal length\\n\n",
    "        or one of the input pairs to be a single value and the other pair to be an array\n",
    "    '''\n",
    "\n",
    "    #Convert decimal degrees to Radians:\n",
    "    lon1 = np.deg2rad(lon1)\n",
    "    lat1 = np.deg2rad(lat1)\n",
    "    lon2 = np.deg2rad(lon2)\n",
    "    lat2 = np.deg2rad(lat2)\n",
    "    # lon1 *= deg2rad_var\n",
    "    # lat1 *= deg2rad_var\n",
    "    # lon2 *= deg2rad_var\n",
    "    # lat2 *= deg2rad_var\n",
    "\n",
    "    #Implementing Haversine Formula: \n",
    "    dlon = np.subtract(lon2, lon1)\n",
    "    #dlon = lon2 - lon1\n",
    "    #dlat = np.subtract(lat2, lat1)\n",
    "\n",
    "    a = np.add(np.multiply(np.sin(lat1), np.sin(lat2)), np.multiply(np.multiply(np.cos(lat1), np.cos(lat2)), np.cos(dlon)))\n",
    "\n",
    "    if rad == True:\n",
    "        return np.arccos(a)\n",
    "    else:\n",
    "        return np.abs(np.arccos(a)/deg2rad_var)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $S_{ij} = \\dfrac{e^{\\frac{ |x_{nu} - x_{psr} | ^ 2}{2 \\sigma^2} } }{2\\pi\\sigma^2}$\n",
    "\n",
    "#### $S_i = \\dfrac{\\sum_j  \\omega_j S_{ij} }{ \\sum_j  \\omega_j }$\n",
    "\n",
    "#### For this project, Cone is $5{\\degree}$\n",
    "\n",
    "#### $B_i = \\dfrac{ \\text{No.of neutrinos within } 5^\\circ \\text{of the declination of the neutrino} }{5 ^\\circ \\text{ Solid angle cone}* \\text{Total no.of neutrinos( = 1134550)}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True, fastmath=True)\n",
    "def S_ij(nu): \n",
    "\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    nu : int\n",
    "        Index of the neutrino in the sample\n",
    "        \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        Returns the signal PDF for the {psrno}th pulsar and nuind_inp neutrino\n",
    "    '''\n",
    "    ang2 = hvovec(msra, msdec, icra[nu], icdec[nu], rad=True) ** 2#, icra[nuind], icdec[nuind], rad=True) ** 2\n",
    "    sg = np.deg2rad(icang[nu]) ** 2\n",
    "    return np.divide(np.exp(-1 * np.divide(ang2, 2*sg)), (2 * np.pi * sg))\n",
    "\n",
    "@jit(nopython=True, fastmath=True)\n",
    "def S_i(nu, all_weights, sum_weights, wall):\n",
    "\n",
    "    '''\n",
    "        Parameters\n",
    "        ----------\n",
    "        nu : int\n",
    "            Index of the neutrino in the sample\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Returns the signal PDF for the {nu}th neutrino with all pulsars \n",
    "        i.e S_i = wieghted-sum_j S_ij\n",
    "    '''\n",
    "\n",
    "\n",
    "\n",
    "    sij = S_ij(nu)\n",
    "    \n",
    "    #sleep(1e-8)\n",
    "    return np.sum(np.multiply(sij, all_weights[wall] / sum_weights[wall]))\n",
    "    #return np.dot(sij, all_weights[wall] ) / sum_weights[wall]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True, fastmath=True)\n",
    "def Bi_stacked(nu, cone=5):\n",
    "\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    nu : int\n",
    "        Index of the neutrino from IceCube sample\n",
    "    cone : float\n",
    "        Cone angle in degrees.\n",
    "    \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Returns the background PDF for the {nu}th neutrino\n",
    "    '''\n",
    "\n",
    "    #count = 0\n",
    "    #count = np.count_nonzero(np.abs(np.subtract(icdec, icdec[nu])) <= cone)\n",
    "    #print(count)\n",
    "    count = 0\n",
    "    for i in range(len(icdec)):\n",
    "        if abs(icdec[i] - icdec[nu]) <= cone:\n",
    "            count+=1\n",
    "    #print(count)\n",
    "    binwidth = (np.sin(np.deg2rad(icdec[nu] + cone)) - np.sin(np.deg2rad(icdec[nu] - cone)))*2*np.pi\n",
    "    #sleep(1e-8)\n",
    "    return count/(binwidth * lnu)\n",
    "    \n",
    "\n",
    "@jit(nopython=True, fastmath=True)\n",
    "def Pr(x, Ns, S, B):\n",
    "\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : int\n",
    "        Assumed no.of associated events\n",
    "\n",
    "    Ns : int\n",
    "        No.of neutrinos used for analysis\n",
    "\n",
    "    S : float\n",
    "        Signal PDF\n",
    "    B : float\n",
    "        Background PDF\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float \n",
    "        Returns the probability of the selected set of neutrinos being associated\\n\n",
    "        with a given pulsar with {Ns} neutrinos, {S} signal and {B} background PDF and {x} assumed associated events\n",
    "    '''\n",
    "\n",
    "    nsN = x/Ns\n",
    "    return np.add(np.multiply(nsN , S), np.multiply(np.subtract(1, nsN), B))\n",
    "\n",
    "\n",
    "@jit(nopython=True, fastmath=True)\n",
    "def wall_nu(nu):\n",
    "    \n",
    "        '''\n",
    "        Parameters\n",
    "        ----------\n",
    "        nu : int\n",
    "            Index of the neutrino from IceCube sample\n",
    "        \n",
    "    \n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            Returns the index of the wall in which the {nu}th neutrino lies\n",
    "        '''\n",
    "        wall = 0\n",
    "        for i in range(len(vec_uptparts)-1):\n",
    "            if vec_uptparts[i] <= nu and vec_uptparts[i+1] > nu:\n",
    "                wall = i\n",
    "                break        \n",
    "        return wall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### weights = \\[$A_{eff}$[i] * $T_{uptime}$[i] * $e_{nu}$ *0.2  for i in range(len($A_{eff}$))\\]\n",
    "\n",
    "#### The weight of a pulsar is constant for a given neutrino sample\n",
    "\n",
    "#### So we can calculate the weight of a pulsar once for all samples (from IC400 to IC86_VII) and store then\n",
    "\n",
    "#### \n",
    "\n",
    "#### \n",
    "\n",
    "#### \n",
    "\n",
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def psr_wt_quick(nusample_wall, psrno, gamma = 1):\n",
    "\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    nusample_wall : float\n",
    "        The index of catalogue of the given neutrino\n",
    "    psrno : int\n",
    "        The index of the pulsar in the ATNF catalogue\n",
    "    gamma : float, optional\n",
    "        (default) 1\n",
    "        The spectral index of the power law\n",
    "    Returns the weight of psrno^th pulsar for a given neutrino sample {nusample_wall}\n",
    "    '''\n",
    "    \n",
    "    #nu_e = icdata['log10(E/GeV)'].values[nu]\n",
    "    psr_decl = icdec[psrno]\n",
    "    #upt = upt_icparts[nusample_wall]\n",
    "    t_upt = upstop_ttt[nusample_wall] - upstart_ttt[nusample_wall]\n",
    "    t_upt*=86400\n",
    "    d_ind = 0\n",
    "\n",
    "    for i in range(len(dec_nu) - 1):\n",
    "        if dec_nu[i] <= psr_decl and dec_nu[i+1] >= psr_decl:\n",
    "            d_ind = i\n",
    "            break\n",
    "    ea_temp = earea[nusample_wall][d_ind*40:(d_ind+1)*40]\n",
    "    \n",
    "    #earea = np.array(ea_temp['A_Eff[cm^2]'].values) * 1e-4\n",
    "    \n",
    "    weight_kj = t_upt * np.sum(np.multiply(ea_temp, np.power(e_nu, gamma)))\n",
    "    \n",
    "    #sleep(1e-8)\n",
    "    \n",
    "    return weight_kj                           \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 36900,\n",
       " 143911,\n",
       " 237044,\n",
       " 373288,\n",
       " 486146,\n",
       " 608687,\n",
       " 735732,\n",
       " 865043,\n",
       " 988700,\n",
       " 1134450]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "icparts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights(gamma):\n",
    "    all_weights= []\n",
    "    for i in range(len(upt_icparts)-1):\n",
    "    #print(upt_icparts[i])\n",
    "        all_weights.append([psr_wt_quick(i, psrno, gamma) for psrno in range(p)])\n",
    "    return np.asfarray(all_weights)\n",
    "all_weights = weights(1)\n",
    "\n",
    "sum_weights = [np.sum(i) for i in all_weights]\n",
    "sum_weights = np.asfarray(sum_weights)\n",
    "cut = 5\n",
    "cone = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_weights = [weights(-1), weights(-2), weights(-2.5), weights(-3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(np.transpose(print_weights[0])).to_csv('/media/darkwake/VIB2/Project-IceCube/task4/outputs/stacking analysis/weights_gamma-1.csv', index=False)\n",
    "pd.DataFrame(np.transpose(print_weights[1])).to_csv('/media/darkwake/VIB2/Project-IceCube/task4/outputs/stacking analysis/weights_gamma-2.csv', index=False)\n",
    "pd.DataFrame(np.transpose(print_weights[2])).to_csv('/media/darkwake/VIB2/Project-IceCube/task4/outputs/stacking analysis/weights_gamma-2.5.csv', index=False)\n",
    "pd.DataFrame(np.transpose(print_weights[3])).to_csv('/media/darkwake/VIB2/Project-IceCube/task4/outputs/stacking analysis/weights_gamma-3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nuind, ang = angfinder(psrno, cut)\n",
    "\n",
    "@jit(nopython=True, cache=True)\n",
    "def sigbag_nu(nu):\n",
    "    '''\n",
    "        Returns the signal and background PDF for the {nu}th neutrino\n",
    "    '''\n",
    "\n",
    "    #sleep(1e-8)\n",
    "\n",
    "    #if nu % 100000 == 0:\n",
    "        #print(nu)\n",
    "        #os.system('clear')\n",
    "    wall = wall_nu(nu)  #Finding the wall/icecube season in which the nu^th neutrino lies\n",
    "    #Ns = vec_uptparts[wall_nu(nu) + 1] - vec_uptparts[wall_nu(nu)]\n",
    "    return [S_i(nu, all_weights, sum_weights, wall)]#,Ns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True, fastmath=True)\n",
    "def TS_st(x, S, B, Ns):\n",
    "\n",
    "    '''\n",
    "    Returns\n",
    "    ----------\n",
    "        Returns the Test Stastic value at\n",
    "        $n_s$ = {x} for its parameters S, B, Ns\n",
    "    '''\n",
    "    if x >=0:\n",
    "        return np.sum(np.asfarray(2*np.log(Pr(x,  Ns, S, B)/B)))\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "def ns_for_TSmax_st(S, B, Ns):\n",
    "    '''\n",
    "    Returns the value of $n_s$ for which\n",
    "    the TS is maximum for {i}^th grb\n",
    "    '''\n",
    "\n",
    "    #returns the TSmax for i^th GRB\n",
    "    nll = lambda x: -TS_st(x, S, B, Ns)\n",
    "    soln = minimize(nll, 3 , bounds = [(0, None)], tol=1e-12)\n",
    "    ns = np.round(soln.x, 6)[0]\n",
    "    #print(soln.success)\n",
    "    return ns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ns = lnu#np.count_nonzero(nuind+1)\n",
    "@njit(parallel=True, fastmath=True)\n",
    "def Ts_arr(x, S_all, B_all, Ns):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : float\n",
    "        The value of x for which the TS is to be calculated\n",
    "\n",
    "    S_all : array\n",
    "        The array of signal PDFs for all the neutrinos\n",
    "\n",
    "    B_all : array\n",
    "        The array of background PDFs for all the neutrinos\n",
    "    \n",
    "    Ns : int\n",
    "        The number of neutrinos\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The TS value for the entire stack of neutrinos for the given value of x\n",
    "    \n",
    "    '''\n",
    "\n",
    "\n",
    "    #Ts_arr = lambda x:\n",
    "    sum = 0.0\n",
    "    for i in range(lnu):\n",
    "        sum += TS_st(x, S_all[i], B_all[i], Ns)\n",
    "    return sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1134450/1134450 [00:18<00:00, 60255.04it/s]\n",
      " 28%|██▊       | 312413/1134450 [00:00<00:01, 500135.66it/s]/tmp/ipykernel_140746/2333230998.py:3: NumbaWarning: \u001b[1mCannot cache compiled function \"sigbag_nu\" as it uses dynamic globals (such as ctypes pointers and large global arrays)\u001b[0m\n",
      "  @jit(nopython=True, cache=True)\n",
      "/tmp/ipykernel_140746/2333230998.py:3: NumbaWarning: \u001b[1mCannot cache compiled function \"sigbag_nu\" as it uses dynamic globals (such as ctypes pointers and large global arrays)\u001b[0m\n",
      "  @jit(nopython=True, cache=True)\n",
      "/tmp/ipykernel_140746/2333230998.py:3: NumbaWarning: \u001b[1mCannot cache compiled function \"sigbag_nu\" as it uses dynamic globals (such as ctypes pointers and large global arrays)\u001b[0m\n",
      "  @jit(nopython=True, cache=True)\n",
      "/tmp/ipykernel_140746/2333230998.py:3: NumbaWarning: \u001b[1mCannot cache compiled function \"sigbag_nu\" as it uses dynamic globals (such as ctypes pointers and large global arrays)\u001b[0m\n",
      "  @jit(nopython=True, cache=True)\n",
      "/tmp/ipykernel_140746/2333230998.py:3: NumbaWarning: \u001b[1mCannot cache compiled function \"sigbag_nu\" as it uses dynamic globals (such as ctypes pointers and large global arrays)\u001b[0m\n",
      "  @jit(nopython=True, cache=True)\n",
      "/tmp/ipykernel_140746/2333230998.py:3: NumbaWarning: \u001b[1mCannot cache compiled function \"sigbag_nu\" as it uses dynamic globals (such as ctypes pointers and large global arrays)\u001b[0m\n",
      "  @jit(nopython=True, cache=True)\n",
      "/tmp/ipykernel_140746/2333230998.py:3: NumbaWarning: \u001b[1mCannot cache compiled function \"sigbag_nu\" as it uses dynamic globals (such as ctypes pointers and large global arrays)\u001b[0m\n",
      "  @jit(nopython=True, cache=True)\n",
      "/tmp/ipykernel_140746/2333230998.py:3: NumbaWarning: \u001b[1mCannot cache compiled function \"sigbag_nu\" as it uses dynamic globals (such as ctypes pointers and large global arrays)\u001b[0m\n",
      "  @jit(nopython=True, cache=True)\n",
      "100%|██████████| 1134450/1134450 [00:19<00:00, 59232.38it/s] \n",
      " 24%|██▍       | 273792/1134450 [00:00<00:01, 511854.11it/s]/tmp/ipykernel_140746/2333230998.py:3: NumbaWarning: \u001b[1mCannot cache compiled function \"sigbag_nu\" as it uses dynamic globals (such as ctypes pointers and large global arrays)\u001b[0m\n",
      "  @jit(nopython=True, cache=True)\n",
      "/tmp/ipykernel_140746/2333230998.py:3: NumbaWarning: \u001b[1mCannot cache compiled function \"sigbag_nu\" as it uses dynamic globals (such as ctypes pointers and large global arrays)\u001b[0m\n",
      "  @jit(nopython=True, cache=True)\n",
      "/tmp/ipykernel_140746/2333230998.py:3: NumbaWarning: \u001b[1mCannot cache compiled function \"sigbag_nu\" as it uses dynamic globals (such as ctypes pointers and large global arrays)\u001b[0m\n",
      "  @jit(nopython=True, cache=True)\n",
      "/tmp/ipykernel_140746/2333230998.py:3: NumbaWarning: \u001b[1mCannot cache compiled function \"sigbag_nu\" as it uses dynamic globals (such as ctypes pointers and large global arrays)\u001b[0m\n",
      "  @jit(nopython=True, cache=True)\n",
      "/tmp/ipykernel_140746/2333230998.py:3: NumbaWarning: \u001b[1mCannot cache compiled function \"sigbag_nu\" as it uses dynamic globals (such as ctypes pointers and large global arrays)\u001b[0m\n",
      "  @jit(nopython=True, cache=True)\n",
      "/tmp/ipykernel_140746/2333230998.py:3: NumbaWarning: \u001b[1mCannot cache compiled function \"sigbag_nu\" as it uses dynamic globals (such as ctypes pointers and large global arrays)\u001b[0m\n",
      "  @jit(nopython=True, cache=True)\n",
      "/tmp/ipykernel_140746/2333230998.py:3: NumbaWarning: \u001b[1mCannot cache compiled function \"sigbag_nu\" as it uses dynamic globals (such as ctypes pointers and large global arrays)\u001b[0m\n",
      "  @jit(nopython=True, cache=True)\n",
      "/tmp/ipykernel_140746/2333230998.py:3: NumbaWarning: \u001b[1mCannot cache compiled function \"sigbag_nu\" as it uses dynamic globals (such as ctypes pointers and large global arrays)\u001b[0m\n",
      "  @jit(nopython=True, cache=True)\n",
      "100%|██████████| 1134450/1134450 [00:21<00:00, 52472.72it/s] \n",
      " 24%|██▍       | 274585/1134450 [00:00<00:02, 427093.54it/s]/tmp/ipykernel_140746/2333230998.py:3: NumbaWarning: \u001b[1mCannot cache compiled function \"sigbag_nu\" as it uses dynamic globals (such as ctypes pointers and large global arrays)\u001b[0m\n",
      "  @jit(nopython=True, cache=True)\n",
      "/tmp/ipykernel_140746/2333230998.py:3: NumbaWarning: \u001b[1mCannot cache compiled function \"sigbag_nu\" as it uses dynamic globals (such as ctypes pointers and large global arrays)\u001b[0m\n",
      "  @jit(nopython=True, cache=True)\n",
      "/tmp/ipykernel_140746/2333230998.py:3: NumbaWarning: \u001b[1mCannot cache compiled function \"sigbag_nu\" as it uses dynamic globals (such as ctypes pointers and large global arrays)\u001b[0m\n",
      "  @jit(nopython=True, cache=True)\n",
      "/tmp/ipykernel_140746/2333230998.py:3: NumbaWarning: \u001b[1mCannot cache compiled function \"sigbag_nu\" as it uses dynamic globals (such as ctypes pointers and large global arrays)\u001b[0m\n",
      "  @jit(nopython=True, cache=True)\n",
      "/tmp/ipykernel_140746/2333230998.py:3: NumbaWarning: \u001b[1mCannot cache compiled function \"sigbag_nu\" as it uses dynamic globals (such as ctypes pointers and large global arrays)\u001b[0m\n",
      "  @jit(nopython=True, cache=True)\n",
      "/tmp/ipykernel_140746/2333230998.py:3: NumbaWarning: \u001b[1mCannot cache compiled function \"sigbag_nu\" as it uses dynamic globals (such as ctypes pointers and large global arrays)\u001b[0m\n",
      "  @jit(nopython=True, cache=True)\n",
      "/tmp/ipykernel_140746/2333230998.py:3: NumbaWarning: \u001b[1mCannot cache compiled function \"sigbag_nu\" as it uses dynamic globals (such as ctypes pointers and large global arrays)\u001b[0m\n",
      "  @jit(nopython=True, cache=True)\n",
      "/tmp/ipykernel_140746/2333230998.py:3: NumbaWarning: \u001b[1mCannot cache compiled function \"sigbag_nu\" as it uses dynamic globals (such as ctypes pointers and large global arrays)\u001b[0m\n",
      "  @jit(nopython=True, cache=True)\n",
      "100%|██████████| 1134450/1134450 [00:20<00:00, 56301.92it/s]\n"
     ]
    }
   ],
   "source": [
    "all_sig_bag = []\n",
    "\n",
    "#gamma = 1\n",
    "all_weights = weights(-1)\n",
    "\n",
    "sum_weights = [np.sum(i) for i in all_weights]\n",
    "sum_weights = np.asfarray(sum_weights)\n",
    "pool = mul.Pool(8, maxtasksperchild=100)\n",
    "op_async = pool.map_async(sigbag_nu, tqdm(range(lnu)))\n",
    "tmp = op_async.get()\n",
    "pool.close()\n",
    "pool = []\n",
    "op_async = []\n",
    "tmp = np.asfarray(tmp)\n",
    "all_sig_bag.append(tmp)\n",
    "\n",
    "#gamma = 2\n",
    "all_weights = weights(-2)\n",
    "\n",
    "sum_weights = [np.sum(i) for i in all_weights]\n",
    "sum_weights = np.asfarray(sum_weights)\n",
    "pool = mul.Pool(8, maxtasksperchild=100)\n",
    "op_async = pool.map_async(sigbag_nu, tqdm(range(lnu)))\n",
    "tmp = op_async.get()\n",
    "pool.close()\n",
    "pool = []\n",
    "op_async = []\n",
    "tmp = np.asfarray(tmp)\n",
    "all_sig_bag.append(tmp)\n",
    "\n",
    "#gamma = -2.5\n",
    "all_weights = weights(-2.5)\n",
    "\n",
    "sum_weights = [np.sum(i) for i in all_weights]\n",
    "sum_weights = np.asfarray(sum_weights)\n",
    "pool = mul.Pool(8, maxtasksperchild=100)\n",
    "op_async = pool.map_async(sigbag_nu, tqdm(range(lnu)))\n",
    "tmp = op_async.get()\n",
    "pool.close()\n",
    "pool = []\n",
    "op_async = []\n",
    "tmp = np.asfarray(tmp)\n",
    "all_sig_bag.append(tmp)\n",
    "\n",
    "\n",
    "\n",
    "#gamma = -3\n",
    "all_weights = weights(-3)\n",
    "\n",
    "sum_weights = [np.sum(i) for i in all_weights]\n",
    "sum_weights = np.asfarray(sum_weights)\n",
    "pool = mul.Pool(8, maxtasksperchild=100)\n",
    "op_async = pool.map_async(sigbag_nu, tqdm(range(lnu)))\n",
    "tmp = op_async.get()\n",
    "pool.close()\n",
    "pool = []\n",
    "op_async = []\n",
    "tmp = np.asfarray(tmp)\n",
    "all_sig_bag.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1134450, 1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_sig_bag[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = [-1, -2, -2.5, -3]\n",
    "for i in range(4):\n",
    "    temp = all_sig_bag[i]\n",
    "    np.savetxt('outputs/sigbag_nu{}.txt'.format(gamma[i]), temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = mul.Pool(8, maxtasksperchild=100)\n",
    "op_star = pool.map_async(Bi_stacked, prange(lnu))\n",
    "all_Bi = op_star.get()\n",
    "pool.close()\n",
    "pool = []\n",
    "op_star =[]\n",
    "all_Bi = np.asarray(all_Bi)\n",
    "np.savetxt('outputs/all_Bi.txt', all_Bi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
